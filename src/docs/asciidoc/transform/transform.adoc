= Transform

Once the raw data is extracted, we can Transform each document into a series of Lucene fields.  
Lucene has a variety of Field Types, each with its own Java class extening oal.Document.Field.  
By transforming the raw data to Lucene Fields, we give Lucene instructions on how to process that particular data element. 
Here decisions must be made as to how the application and its users will interact with data from Lucene.  
For our examples, we have an interface DocumentTransformer and then we can create implementations
based on our application and user needs.

link:../apidocs/j/lucene/tutorial/transform/impl/package-summary.html[javadoc]

== DocumentTransformerHtmlBibleImpl

Raw field "chapter" becomes an IntField, "source" and "book" become String fields and "add_timestamp"
becomes a LongField, with the epoch milliseconds.  All of these will be single-token fields and when indexed, 
will need to be matched exactly.  Each of these will produce a single token making them elgible to be additionally indexed as DocValues. 
DocValue fields allow efficient sorting and faceting and the tokens themselves can be efficiently returned back to the user with the search results.
 
Raw field "keywords" is indexed as multiple tokens, one per keyword in the source data.  Note some of these
keywords include whitespace, and queries will need to match these tokens exactly, including any whitespace. 
We used Lucene's KeywordField type, which is similar to StringField but without scoring.  Unlike StringField, matches on a KeywordField will
not contribute to a document's score, or placement within the search results.  Any field with multiple tokens cannot be 
encoded as DocValues, so these cannot be used for sorting or faceting.  In order to make the data available for display, we include a separate stored field.

Raw field "synopsis" is indexed as a "text" field, being unstructured text.  
Text fields require a Lucene Analyzer to determine how to break the text into Tokens.  Here we use the
StandardAnalyzer.  This analyzer uses oal.analysis.standard.StandardTokenizer, for which the javadoc links to
Unicode Standard Annex #29 for the token-breaking rules.  Mostly, this means that Standard Analyzer splits tokens on whitespace. 
Standard Analyzer also converts all text to lowercase and has the ability to omit common words from indexing (Stop Words).
In our case, we do not specify any Stop Words as their usage is often not advised for best results.

The main content is in raw field "text".  We also index this as a "text" field but in this case use the option to provide a java.io.Reader 
instead of a String.  Using a Reader would allow us to stream data directly from some other persistent source.  In this case, we 
merely use java.io.StringReader and then wrap this in Lucene's oal.analysys.charfilter.HTMLStripCharFilter.  All Lucene Char Filters extend
java.io.Reader and serve to pre-process the data.  In this case, we remove HTML from the raw data before StandardAnalyzer processes it.


link:../../src/test/java/j/lucene/tutorial/transform/impl/DocumentTransformerHtmlBibleImplTest.java[unit test]
